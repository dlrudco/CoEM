{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrast Usage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-b600d4e46b9d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'../Data/quickdraw'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m \u001B[0mdataloaders\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset_sizes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mquickdraw_setter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m2048\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_workers\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[0mdevice\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'cuda:0'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\PyCharmProjects\\gct634_final_project\\image_classifier\\data_utils\\datasetter.py\u001B[0m in \u001B[0;36mquickdraw_setter\u001B[1;34m(root, batch_size, num_workers)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[1;31m# Datasets\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 26\u001B[1;33m     \u001B[0mtrain_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mQuickDraw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrain_transforms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# train transform applied\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m     \u001B[0mtest_set\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mQuickDraw\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtest_transforms\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# test transform applied\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\PyCharmProjects\\gct634_final_project\\image_classifier\\data_utils\\QuickDraw.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, root, train, transform, using_classes)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     23\u001B[0m         \u001B[1;31m# get data from root\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 24\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_images_labels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mclasses\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;31m# process data\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mE:\\PyCharmProjects\\gct634_final_project\\image_classifier\\data_utils\\QuickDraw.py\u001B[0m in \u001B[0;36m_images_labels\u001B[1;34m(self, classes)\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_npy\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m100000\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m         \u001B[0mimages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;36m255.0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     54\u001B[0m         \u001B[0mimages\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mimages\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m28\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLongTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_utils import *\n",
    "from train_tools import *\n",
    "\n",
    "root = '../Data/quickdraw'\n",
    "dataloaders, dataset_sizes = quickdraw_setter(root=root, batch_size=2048, num_workers=0)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet was made\n"
     ]
    }
   ],
   "source": [
    "images, v_labels = next(iter(dataloaders['train']))\n",
    "\n",
    "model = resnet20(dim_in=1, num_classes=20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.to(device)\n",
    "    images, v_labels = images.to(device), v_labels.to(device)\n",
    "    _, v_features = model(images, get_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fake audio features & audio labels\n",
    "a_features, a_labels = torch.randn(256, 128).to(device), next(iter(dataloaders['train']))[1][:256].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 128])\n",
      "torch.Size([256])\n",
      "torch.Size([2048, 128])\n",
      "torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "# Remind that audio & visual batch size can differ\n",
    "print(a_features.shape)\n",
    "print(a_labels.shape)\n",
    "print(v_features.shape)\n",
    "print(v_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6381, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "criterion = AVContrastLoss()\n",
    "loss = criterion(a_features, a_labels, v_features, v_labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Save v_features & v_labels to npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_concater(self, tensor1, tensor2):\n",
    "    if tensor1 is None:\n",
    "        tensor1 = tensor2\n",
    "    else:\n",
    "        tensor1 = torch.cat((tensor1, tensor2), dim=0)\n",
    "\n",
    "    return tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet was made\n",
      "torch.Size([1990000, 128])\n",
      "torch.Size([1990000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from data_utils import *\n",
    "from train_tools import *\n",
    "\n",
    "\n",
    "def tensor_concater(tensor1, tensor2):\n",
    "    if tensor1 is None:\n",
    "        tensor1 = tensor2\n",
    "    else:\n",
    "        tensor1 = torch.cat((tensor1, tensor2), dim=0)\n",
    "\n",
    "    return tensor1\n",
    "\n",
    "\n",
    "root = '../Data/quickdraw'\n",
    "dataloaders, dataset_sizes = quickdraw_setter(root=root, batch_size=2048, num_workers=10)\n",
    "device = 'cuda:0'\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = resnet20(dim_in=1, num_classes=20).to(device)\n",
    "\n",
    "images_cat, labels_cat = None, None\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load('./results/res20_0.8982.pth'))\n",
    "    for images, v_labels in dataloaders['train']:\n",
    "        images = images.to(device)\n",
    "        _, v_features = model(images, get_features=True)\n",
    "        images_cat = tensor_concater(images_cat, v_features.cpu())\n",
    "        labels_cat = tensor_concater(labels_cat, v_labels.cpu())\n",
    "        \n",
    "print(images_cat.shape)\n",
    "print(labels_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save\n",
    "\n",
    "save('./v_features.npy', images_cat.numpy())\n",
    "save('./v_labels.npy', labels_cat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1990000, 128)\n",
      "(1990000,)\n"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "\n",
    "v_features_load = load('./v_features.npy')\n",
    "print(v_features_load.shape)\n",
    "\n",
    "v_labels_load = load('./v_labels.npy')\n",
    "print(v_labels_load.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}